from langchain_core.runnables import RunnableLambda, RunnableMap, RunnableSequence
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import Milvus
from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings
from langchain_openai import ChatOpenAI
from operator import itemgetter

# ğŸ“¥ Embeddings TEI local (rÃ©seau docker -> nom de service)
embedding = HuggingFaceEndpointEmbeddings(model="http://tei:8010")

# ğŸ§  Vector store Milvus (collection crÃ©Ã©e par RAG_tools)
vectorstore = Milvus(
    embedding_function=embedding,
    connection_args={"host": "localhost", "port": "19530"},
    collection_name="rag_demo",
)
retriever = vectorstore.as_retriever()

# ğŸ§¾ Prompt de rÃ©ponse fondÃ©e sur le contexte
template = """Tu es un expert. Utilise le contexte suivant pour rÃ©pondre Ã  la question.

Contexte :
{context}

Question :
{question}

RÃ©ponse :"""
prompt = PromptTemplate.from_template(template)

# ğŸ¤– LLM via vLLM exposÃ© en HTTP (attention : chemin complet !)
llm = ChatOpenAI(
    base_url="http://localhost:8000/v1/chat/completions",
    api_key="not-needed",
    model="auto"
)

# ğŸ” ChaÃ®ne RAG LCEL : {question} -> retriever -> prompt -> llm -> str
rag_chain: RunnableSequence = (
    RunnableMap({
        "context": itemgetter("question") | retriever,
        "question": itemgetter("question")
    }) | prompt | llm | StrOutputParser()
)

# ---------------------------------------------------------------------
# Interface imposÃ©e par l'UI : run_chain(prompt, history) -> str
def run_chain(user_input: str, _history=None, model_path=None) -> str:
    return rag_chain.invoke({"question": user_input})
# ---------------------------------------------------------------------

# ğŸ§ª ExÃ©cution isolÃ©e
if __name__ == "__main__":
    q = "Qui est le personnage principal ?"
    print("[TEST] Question :", q)
    print("[TEST] RÃ©ponse :", run_chain(q))
